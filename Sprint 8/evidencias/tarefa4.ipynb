{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecffc6f3-66e0-4aab-9fe8-b512549796ab",
   "metadata": {},
   "source": [
    "### Perguntas dessa tarefa:\n",
    "\n",
    "1. Inicialmente iremos preparar o ambiente, definindo o diretório onde nosso código será desenvolvido. Para este diretório iremos copiar o arquivo nomes_aleatorios.txt.\n",
    "\n",
    "   Após, em nosso script Python, devemos importar as bibliotecas necessárias:\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "   from pyspark import SparkContext, SQLContext\n",
    "   ```\n",
    "   Aplicando as bibliotecas do Spark, podemos definir a Spark Session e sobre ela definir o Context para habilitar o módulo SQL\n",
    "   ```python\n",
    "   spark = SparkSession \\\n",
    "                   .builder \\\n",
    "                   .master(\"local[*]\")\\\n",
    "                   .appName(\"Exercicio Intro\") \\\n",
    "                   .getOrCreate()\n",
    "   ```\n",
    "   Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando `spark.read.csv`. Carregue-o para dentro de um dataframe chamado `df_nomes` e, por fim, liste algumas linhas através do método `show`. Exemplo: `df_nomes.show(5)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ffc2eb2-4e58-4bc0-8cd9-f92b0076b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             _c0|\n",
      "+----------------+\n",
      "|  Frances Bennet|\n",
      "|   Jamie Russell|\n",
      "|  Edward Kistler|\n",
      "|   Sheila Maurer|\n",
      "|Donald Golightly|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "\n",
    "# Inicializando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1. Lendo o arquivo nomes_aleatorios.txt\n",
    "df_nomes = spark.read.csv(\"nomes_aleatorios.txt\", header=False, inferSchema=True)\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40adfabf-f818-4383-8994-2bd38b522ad0",
   "metadata": {},
   "source": [
    "2. No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo `df_nomes.nome`) ou por índice (`df_nomes['nome']`). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.\n",
    "\n",
    "   Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método `df_nomes.printSchema()`.\n",
    "\n",
    "   Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bce87c1c-9445-47c2-981d-2dca3995f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            Nomes|\n",
      "+-----------------+\n",
      "|   Frances Bennet|\n",
      "|    Jamie Russell|\n",
      "|   Edward Kistler|\n",
      "|    Sheila Maurer|\n",
      "| Donald Golightly|\n",
      "|       David Gray|\n",
      "|      Joy Bennett|\n",
      "|      Paul Kriese|\n",
      "|Berniece Ornellas|\n",
      "|    Brian Farrell|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df870a-6093-4c7e-a75a-38e46a98e5f0",
   "metadata": {},
   "source": [
    "3. Ao dataframe (`df_nomes`), adicione nova coluna chamada `Escolaridade` e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Médio ou Superior.\n",
    "   Para esta etapa, evite usar funções de iteração, como por exemplo: `for`, `while`, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f91f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+---------+\n",
      "|Escolaridade|  count|Proporção|\n",
      "+------------+-------+---------+\n",
      "| Fundamental|3334349|0.3334349|\n",
      "|       Médio|3329643|0.3329643|\n",
      "|    Superior|3336008|0.3336008|\n",
      "+------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3. Atribuindo valores de escolaridade com base em um único número aleatório por registro\n",
    "df_nomes = df_nomes.withColumn(\"Random\", rand())\n",
    "\n",
    "# Definindo os limites para cada valor de escolaridade\n",
    "fundamental_threshold = 1/3\n",
    "medio_threshold = 2/3\n",
    "\n",
    "# Atribuindo valores de escolaridade com base nos limites\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\",\n",
    "                               when(df_nomes[\"Random\"] <= fundamental_threshold, \"Fundamental\")\n",
    "                               .when((df_nomes[\"Random\"] > fundamental_threshold) & (df_nomes[\"Random\"] <= medio_threshold), \"Médio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Removendo a coluna \"Random\" do DataFrame\n",
    "df_nomes = df_nomes.drop(\"Random\")\n",
    "\n",
    "# Contando o número de ocorrências de cada valor de escolaridade no DataFrame\n",
    "escolaridade_counts = df_nomes.groupBy(\"Escolaridade\").count().orderBy(\"Escolaridade\")\n",
    "\n",
    "# Calculando a proporção de ocorrências de cada valor de escolaridade\n",
    "total_count = df_nomes.count()\n",
    "escolaridade_proportion = escolaridade_counts.withColumn(\"Proporção\", escolaridade_counts[\"count\"] / total_count)\n",
    "\n",
    "# Exibindo a proporção de ocorrências de cada valor de escolaridade\n",
    "escolaridade_proportion.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19cad2-6cbb-406e-accd-36350b0c83e6",
   "metadata": {},
   "source": [
    "4. Ao dataframe (`df_nomes`), adicione nova coluna chamada `Pais` e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
    "   Para esta etapa, evite usar funções de iteração, como por exemplo: `for`, `while`, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d16db47-3c1b-4de7-b9a3-2d11babe3cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/24 18:07:27 WARN SimpleFunctionRegistry: The function random_country replaced a previously registered function.\n",
      "[Stage 184:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+---------+\n",
      "|           Pais| count|Proporção|\n",
      "+---------------+------+---------+\n",
      "|      Argentina|769483|0.0769483|\n",
      "|        Bolívia|770986|0.0770986|\n",
      "|         Brasil|767923|0.0767923|\n",
      "|          Chile|769906|0.0769906|\n",
      "|       Colômbia|768277|0.0768277|\n",
      "|        Equador|768299|0.0768299|\n",
      "|         Guiana|768394|0.0768394|\n",
      "|Guiana Francesa|768487|0.0768487|\n",
      "|       Paraguai|769967|0.0769967|\n",
      "|           Peru|770242|0.0770242|\n",
      "|       Suriname|769183|0.0769183|\n",
      "|        Uruguai|769942|0.0769942|\n",
      "|      Venezuela|768911|0.0768911|\n",
      "+---------------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. Definindo a lista de países da América do Sul \n",
    "paises_americadosul = ['Argentina', 'Bolívia', 'Brasil', 'Chile', 'Colômbia', 'Equador', 'Guiana', 'Guiana Francesa', 'Paraguai', 'Peru', 'Suriname', 'Uruguai', 'Venezuela']\n",
    "\n",
    "# Função UDF para selecionar um país aleatório\n",
    "def random_country():\n",
    "    return random.choice(paises_americadosul)\n",
    "\n",
    "# Registrando a função UDF\n",
    "spark.udf.register(\"random_country\", random_country, StringType())\n",
    "\n",
    "# Adicionando a nova coluna 'Pais' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", expr(\"random_country()\"))\n",
    "\n",
    "# Contando o número de ocorrências de cada país no DataFrame\n",
    "counts = df_nomes.groupBy(\"Pais\").count().orderBy(\"Pais\")\n",
    "\n",
    "# Calculando a proporção de ocorrências de cada país\n",
    "total_count = df_nomes.count()\n",
    "proportion = counts.withColumn(\"Proporção\", F.col(\"count\") / total_count)\n",
    "\n",
    "# Exibindo a proporção\n",
    "proportion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4ed59-5872-4478-aaca-14875588adec",
   "metadata": {},
   "source": [
    "5. Ao dataframe (`df_nomes`), adicione nova coluna chamada `AnoNascimento` e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória.\n",
    "   Para esta etapa, evite usar funções de iteração, como por exemplo: `for`, `while`, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52f94859-7b92-4284-ae08-c53a223c02f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+---------+-------------+\n",
      "|            Nomes|Escolaridade|     Pais|AnoNascimento|\n",
      "+-----------------+------------+---------+-------------+\n",
      "|   Frances Bennet| Fundamental| Paraguai|         1985|\n",
      "|    Jamie Russell|    Superior|  Equador|         1978|\n",
      "|   Edward Kistler|       Médio|Argentina|         1999|\n",
      "|    Sheila Maurer| Fundamental|Venezuela|         1962|\n",
      "| Donald Golightly|    Superior|  Bolívia|         1989|\n",
      "|       David Gray|    Superior|  Uruguai|         1979|\n",
      "|      Joy Bennett|       Médio|     Peru|         2010|\n",
      "|      Paul Kriese|    Superior| Colômbia|         1949|\n",
      "|Berniece Ornellas|       Médio| Paraguai|         2008|\n",
      "|    Brian Farrell|       Médio|  Uruguai|         1997|\n",
      "+-----------------+------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Adicionando a coluna 'AnoNascimento' com valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (lit(1945) + (rand(seed=42) * (2010 - 1945 + 1)).cast(\"int\")))\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7e2ba-8745-499a-8043-c53147f2ed3b",
   "metadata": {},
   "source": [
    "6. Usando o método `select` do dataframe (`df_nomes`), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado `df_select` e mostre 10 nomes deste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbde081a-18fb-4483-a215-a00b9f2e43f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pessoas que nasceram neste século:\n",
      "+-----------------+-------------+\n",
      "|            Nomes|AnoNascimento|\n",
      "+-----------------+-------------+\n",
      "|      Joy Bennett|         2010|\n",
      "|Berniece Ornellas|         2008|\n",
      "|      Albert Leef|         2000|\n",
      "|     Rebecca Snow|         2003|\n",
      "|  Kenneth Rayburn|         2001|\n",
      "|    Milton Dillon|         2002|\n",
      "|       Ned Tester|         2010|\n",
      "|    Lynne Dustman|         2003|\n",
      "|    George Miller|         2002|\n",
      "| Cristina Sheston|         2006|\n",
      "+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Selecionando as pessoas que nasceram neste século\n",
    "df_select = df_nomes.filter(df_nomes[\"AnoNascimento\"] >= 2000)\n",
    "print(\"Pessoas que nasceram neste século:\")\n",
    "df_select.select(\"Nomes\", \"AnoNascimento\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6e919-2023-4760-9627-4f3161d7a357",
   "metadata": {},
   "source": [
    "7. Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
    "\n",
    "   ```python\n",
    "   df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "   spark.sql(\"select * from pessoas\").show()\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cab8a758-293f-4d49-9f82-b4cee1712088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pessoas que nasceram neste século (usando Spark SQL):\n",
      "+-----------------+-------------+\n",
      "|            Nomes|AnoNascimento|\n",
      "+-----------------+-------------+\n",
      "|      Joy Bennett|         2010|\n",
      "|Berniece Ornellas|         2008|\n",
      "|      Albert Leef|         2000|\n",
      "|     Rebecca Snow|         2003|\n",
      "|  Kenneth Rayburn|         2001|\n",
      "|    Milton Dillon|         2002|\n",
      "|       Ned Tester|         2010|\n",
      "|    Lynne Dustman|         2003|\n",
      "|    George Miller|         2002|\n",
      "| Cristina Sheston|         2006|\n",
      "+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Usando Spark SQL para selecionar as pessoas que nasceram neste século\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "df_select_sql = spark.sql(\"SELECT Nomes, AnoNascimento FROM pessoas WHERE AnoNascimento >= 2000\")\n",
    "print(\"Pessoas que nasceram neste século (usando Spark SQL):\")\n",
    "df_select_sql.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d77f67-cd6c-458b-a6ff-e306fb54c4f6",
   "metadata": {},
   "source": [
    "8. Usando o método `select` do Dataframe `df_nomes`, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87b63188-724a-433a-88d7-75df0e3481a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de pessoas da geração Millennials: 2270069\n"
     ]
    }
   ],
   "source": [
    "# 8. Contando o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994)\n",
    "count_millennials = df_nomes.filter((df_nomes[\"AnoNascimento\"] >= 1980) & (df_nomes[\"AnoNascimento\"] <= 1994)).count()\n",
    "print(\"Número de pessoas da geração Millennials:\", count_millennials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71422b6a-4fba-4ce1-b354-5150279a6479",
   "metadata": {},
   "source": [
    "9. Repita o processo da Pergunta 8 utilizando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c4ee6cb-8299-4beb-9a05-af4aae6e38d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de pessoas da geração Millennials (usando Spark SQL): 2270069\n"
     ]
    }
   ],
   "source": [
    "# 9. Repetindo o processo da Pergunta 8 utilizando Spark SQL\n",
    "count_millennials_sql = spark.sql(\"SELECT COUNT(*) FROM pessoas WHERE AnoNascimento >= 1980 AND AnoNascimento <= 1994\").collect()[0][0]\n",
    "print(\"Número de pessoas da geração Millennials (usando Spark SQL):\", count_millennials_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d73e9a-c84d-41bf-acc5-92c1743b9f5f",
   "metadata": {},
   "source": [
    "10. Usando Spark SQL, obtenha a quantidade de pessoas de cada país para uma das gerações abaixo. Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
    "\n",
    "   - Baby Boomers – nascidos entre 1944 e 1964;\n",
    "   - Geração X – nascidos entre 1965 e 1979;\n",
    "   - Millennials (Geração Y) – nascidos entre 1980 e 1994;\n",
    "   - Geração Z – nascidos entre 1995 e 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d265830-bfbb-4e52-a62c-0a09bf71f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 202:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+\n",
      "|Pais           |Geracao     |Quantidade|\n",
      "+---------------+------------+----------+\n",
      "|Argentina      |Baby Boomers|233113    |\n",
      "|Argentina      |Geração X   |175051    |\n",
      "|Argentina      |Geração Z   |186373    |\n",
      "|Argentina      |Millennials |174270    |\n",
      "|Bolívia        |Baby Boomers|233588    |\n",
      "|Bolívia        |Geração X   |174986    |\n",
      "|Bolívia        |Geração Z   |186126    |\n",
      "|Bolívia        |Millennials |174730    |\n",
      "|Brasil         |Baby Boomers|234191    |\n",
      "|Brasil         |Geração X   |174648    |\n",
      "|Brasil         |Geração Z   |186374    |\n",
      "|Brasil         |Millennials |174405    |\n",
      "|Chile          |Baby Boomers|233165    |\n",
      "|Chile          |Geração X   |174762    |\n",
      "|Chile          |Geração Z   |186780    |\n",
      "|Chile          |Millennials |175132    |\n",
      "|Colômbia       |Baby Boomers|233257    |\n",
      "|Colômbia       |Geração X   |175018    |\n",
      "|Colômbia       |Geração Z   |186078    |\n",
      "|Colômbia       |Millennials |174915    |\n",
      "|Equador        |Baby Boomers|233147    |\n",
      "|Equador        |Geração X   |174561    |\n",
      "|Equador        |Geração Z   |186700    |\n",
      "|Equador        |Millennials |174884    |\n",
      "|Guiana         |Baby Boomers|233407    |\n",
      "|Guiana         |Geração X   |174928    |\n",
      "|Guiana         |Geração Z   |186382    |\n",
      "|Guiana         |Millennials |173836    |\n",
      "|Guiana Francesa|Baby Boomers|232662    |\n",
      "|Guiana Francesa|Geração X   |174847    |\n",
      "|Guiana Francesa|Geração Z   |186426    |\n",
      "|Guiana Francesa|Millennials |174485    |\n",
      "|Paraguai       |Baby Boomers|233845    |\n",
      "|Paraguai       |Geração X   |174746    |\n",
      "|Paraguai       |Geração Z   |187315    |\n",
      "|Paraguai       |Millennials |175062    |\n",
      "|Peru           |Baby Boomers|233189    |\n",
      "|Peru           |Geração X   |174637    |\n",
      "|Peru           |Geração Z   |186620    |\n",
      "|Peru           |Millennials |174574    |\n",
      "|Suriname       |Baby Boomers|232545    |\n",
      "|Suriname       |Geração X   |175235    |\n",
      "|Suriname       |Geração Z   |186745    |\n",
      "|Suriname       |Millennials |174229    |\n",
      "|Uruguai        |Baby Boomers|233082    |\n",
      "|Uruguai        |Geração X   |175060    |\n",
      "|Uruguai        |Geração Z   |186540    |\n",
      "|Uruguai        |Millennials |175146    |\n",
      "|Venezuela      |Baby Boomers|232613    |\n",
      "|Venezuela      |Geração X   |174807    |\n",
      "|Venezuela      |Geração Z   |186382    |\n",
      "|Venezuela      |Millennials |174401    |\n",
      "+---------------+------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Registrando o DataFrame como uma tabela temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Escrevendo uma consulta SQL para calcular a quantidade de pessoas de cada país para cada geração\n",
    "consulta = \"\"\"\n",
    "SELECT Pais,\n",
    "       CASE WHEN AnoNascimento BETWEEN 1944 AND 1964 THEN 'Baby Boomers'\n",
    "            WHEN AnoNascimento BETWEEN 1965 AND 1979 THEN 'Geração X'\n",
    "            WHEN AnoNascimento BETWEEN 1980 AND 1994 THEN 'Millennials'\n",
    "            WHEN AnoNascimento BETWEEN 1995 AND 2015 THEN 'Geração Z'\n",
    "            ELSE 'Outros' END AS Geracao,\n",
    "       COUNT(*) AS Quantidade\n",
    "FROM pessoas\n",
    "GROUP BY Pais, Geracao\n",
    "ORDER BY Pais, Geracao, Quantidade\n",
    "\"\"\"\n",
    "\n",
    "# Executando a consulta SQL e armazenando o resultado em um novo DataFrame\n",
    "df_resultado = spark.sql(consulta)\n",
    "\n",
    "# Exibindo todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
    "df_resultado.show(df_resultado.count(), truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
