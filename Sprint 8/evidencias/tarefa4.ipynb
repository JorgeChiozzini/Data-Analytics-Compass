{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecffc6f3-66e0-4aab-9fe8-b512549796ab",
   "metadata": {},
   "source": [
    "### Perguntas dessa tarefa:\n",
    "\n",
    "1. Inicialmente iremos preparar o ambiente, definindo o diretório onde nosso código será desenvolvido. Para este diretório iremos copiar o arquivo nomes_aleatorios.txt.\n",
    "\n",
    "   Após, em nosso script Python, devemos importar as bibliotecas necessárias:\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "   from pyspark import SparkContext, SQLContext\n",
    "   ```\n",
    "   Aplicando as bibliotecas do Spark, podemos definir a Spark Session e sobre ela definir o Context para habilitar o módulo SQL\n",
    "   ```python\n",
    "   spark = SparkSession \\\n",
    "                   .builder \\\n",
    "                   .master(\"local[*]\")\\\n",
    "                   .appName(\"Exercicio Intro\") \\\n",
    "                   .getOrCreate()\n",
    "   ```\n",
    "   Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando `spark.read.csv`. Carregue-o para dentro de um dataframe chamado `df_nomes` e, por fim, liste algumas linhas através do método `show`. Exemplo: `df_nomes.show(5)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ffc2eb2-4e58-4bc0-8cd9-f92b0076b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             _c0|\n",
      "+----------------+\n",
      "|  Frances Bennet|\n",
      "|   Jamie Russell|\n",
      "|  Edward Kistler|\n",
      "|   Sheila Maurer|\n",
      "|Donald Golightly|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, when, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "\n",
    "# Inicializando a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1. Lendo o arquivo nomes_aleatorios.txt\n",
    "df_nomes = spark.read.csv(\"nomes_aleatorios.txt\", header=False, inferSchema=True)\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40adfabf-f818-4383-8994-2bd38b522ad0",
   "metadata": {},
   "source": [
    "2. No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo `df_nomes.nome`) ou por índice (`df_nomes['nome']`). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.\n",
    "\n",
    "   Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método `df_nomes.printSchema()`.\n",
    "\n",
    "   Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bce87c1c-9445-47c2-981d-2dca3995f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            Nomes|\n",
      "+-----------------+\n",
      "|   Frances Bennet|\n",
      "|    Jamie Russell|\n",
      "|   Edward Kistler|\n",
      "|    Sheila Maurer|\n",
      "| Donald Golightly|\n",
      "|       David Gray|\n",
      "|      Joy Bennett|\n",
      "|      Paul Kriese|\n",
      "|Berniece Ornellas|\n",
      "|    Brian Farrell|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Renomeando a coluna para 'Nomes'\n",
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df870a-6093-4c7e-a75a-38e46a98e5f0",
   "metadata": {},
   "source": [
    "3. Ao dataframe (`df_nomes`), adicione nova coluna chamada `Escolaridade` e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Médio ou Superior.\n",
    "   Para esta etapa, evite usar funções de iteração, como por exemplo: `for`, `while`, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "46e36760-68df-4838-bea5-56b4a5ca6ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|            Nomes|Escolaridade|\n",
      "+-----------------+------------+\n",
      "|   Frances Bennet|    Superior|\n",
      "|    Jamie Russell|       Médio|\n",
      "|   Edward Kistler|    Superior|\n",
      "|    Sheila Maurer| Fundamental|\n",
      "| Donald Golightly|       Médio|\n",
      "|       David Gray| Fundamental|\n",
      "|      Joy Bennett|       Médio|\n",
      "|      Paul Kriese|    Superior|\n",
      "|Berniece Ornellas|       Médio|\n",
      "|    Brian Farrell| Fundamental|\n",
      "+-----------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Adicionando a nova coluna 'Escolaridade' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", when(rand() < 0.33, \"Fundamental\").\n",
    "                                              when(rand() < 0.66, \"Médio\").\n",
    "                                              otherwise(\"Superior\"))\n",
    "\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19cad2-6cbb-406e-accd-36350b0c83e6",
   "metadata": {},
   "source": [
    "4. Ao dataframe (`df_nomes`), adicione nova coluna chamada `Pais` e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
    "   Para esta etapa, evite usar funções de iteração, como por exemplo: `for`, `while`, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d16db47-3c1b-4de7-b9a3-2d11babe3cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+---------+\n",
      "|            Nomes|Escolaridade|     Pais|\n",
      "+-----------------+------------+---------+\n",
      "|   Frances Bennet|    Superior|   Brasil|\n",
      "|    Jamie Russell|       Médio|   Brasil|\n",
      "|   Edward Kistler|    Superior|Venezuela|\n",
      "|    Sheila Maurer| Fundamental| Paraguai|\n",
      "| Donald Golightly|       Médio|  Bolívia|\n",
      "|       David Gray| Fundamental|  Bolívia|\n",
      "|      Joy Bennett|       Médio| Colômbia|\n",
      "|      Paul Kriese|    Superior|    Chile|\n",
      "|Berniece Ornellas|       Médio|     Peru|\n",
      "|    Brian Farrell| Fundamental|   Brasil|\n",
      "+-----------------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Definindo a lista de países da América do Sul \n",
    "paises_americadosul = ['Argentina', 'Bolívia', 'Brasil', 'Chile', 'Colômbia', 'Equador', 'Guiana', 'Paraguai', 'Peru', 'Suriname', 'Uruguai', 'Venezuela']\n",
    "escolaridades = ['Fundamental', 'Médio', 'Superior']\n",
    "\n",
    "# Função UDF para selecionar um país aleatório\n",
    "def random_country():\n",
    "    return random.choice(paises_americadosul)\n",
    "\n",
    "# Registrando a função UDF\n",
    "spark.udf.register(\"random_country\", random_country, StringType())\n",
    "\n",
    "# Adicionando a nova coluna 'Pais' com valores aleatórios\n",
    "df_nomes = df_nomes.withColumn(\"Pais\", expr(\"random_country()\"))\n",
    "\n",
    "# Exibindo o DataFrame resultante\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4ed59-5872-4478-aaca-14875588adec",
   "metadata": {},
   "source": [
    "5. Ao dataframe (`df_nomes`), adicione nova coluna chamada `AnoNascimento` e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória.\n",
    "   Para esta etapa, evite usar funções de iteração, como por exemplo: `for`, `while`, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52f94859-7b92-4284-ae08-c53a223c02f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+---------+-------------+\n",
      "|            Nomes|Escolaridade|     Pais|AnoNascimento|\n",
      "+-----------------+------------+---------+-------------+\n",
      "|   Frances Bennet|    Superior|Venezuela|         1985|\n",
      "|    Jamie Russell|       Médio|  Uruguai|         1978|\n",
      "|   Edward Kistler|    Superior|   Guiana|         1999|\n",
      "|    Sheila Maurer| Fundamental| Paraguai|         1962|\n",
      "| Donald Golightly|       Médio|     Peru|         1989|\n",
      "|       David Gray| Fundamental|  Bolívia|         1979|\n",
      "|      Joy Bennett|       Médio| Suriname|         2010|\n",
      "|      Paul Kriese|    Superior|    Chile|         1949|\n",
      "|Berniece Ornellas|       Médio|Argentina|         2008|\n",
      "|    Brian Farrell| Fundamental| Colômbia|         1997|\n",
      "+-----------------+------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Adicionando a coluna 'AnoNascimento' com valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (lit(1945) + (rand(seed=42) * (2010 - 1945 + 1)).cast(\"int\")))\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7e2ba-8745-499a-8043-c53147f2ed3b",
   "metadata": {},
   "source": [
    "6. Usando o método `select` do dataframe (`df_nomes`), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado `df_select` e mostre 10 nomes deste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dbde081a-18fb-4483-a215-a00b9f2e43f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pessoas que nasceram neste século:\n",
      "+-----------------+-------------+\n",
      "|            Nomes|AnoNascimento|\n",
      "+-----------------+-------------+\n",
      "|      Joy Bennett|         2010|\n",
      "|Berniece Ornellas|         2008|\n",
      "|      Albert Leef|         2000|\n",
      "|     Rebecca Snow|         2003|\n",
      "|  Kenneth Rayburn|         2001|\n",
      "|    Milton Dillon|         2002|\n",
      "|       Ned Tester|         2010|\n",
      "|    Lynne Dustman|         2003|\n",
      "|    George Miller|         2002|\n",
      "| Cristina Sheston|         2006|\n",
      "+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Selecionando as pessoas que nasceram neste século\n",
    "df_select = df_nomes.filter(df_nomes[\"AnoNascimento\"] >= 2000)\n",
    "print(\"Pessoas que nasceram neste século:\")\n",
    "df_select.select(\"Nomes\", \"AnoNascimento\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6e919-2023-4760-9627-4f3161d7a357",
   "metadata": {},
   "source": [
    "7. Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
    "\n",
    "   ```python\n",
    "   df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "   spark.sql(\"select * from pessoas\").show()\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cab8a758-293f-4d49-9f82-b4cee1712088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pessoas que nasceram neste século (usando Spark SQL):\n",
      "+-----------------+-------------+\n",
      "|            Nomes|AnoNascimento|\n",
      "+-----------------+-------------+\n",
      "|      Joy Bennett|         2010|\n",
      "|Berniece Ornellas|         2008|\n",
      "|      Albert Leef|         2000|\n",
      "|     Rebecca Snow|         2003|\n",
      "|  Kenneth Rayburn|         2001|\n",
      "|    Milton Dillon|         2002|\n",
      "|       Ned Tester|         2010|\n",
      "|    Lynne Dustman|         2003|\n",
      "|    George Miller|         2002|\n",
      "| Cristina Sheston|         2006|\n",
      "+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Usando Spark SQL para selecionar as pessoas que nasceram neste século\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "df_select_sql = spark.sql(\"SELECT Nomes, AnoNascimento FROM pessoas WHERE AnoNascimento >= 2000\")\n",
    "print(\"Pessoas que nasceram neste século (usando Spark SQL):\")\n",
    "df_select_sql.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d77f67-cd6c-458b-a6ff-e306fb54c4f6",
   "metadata": {},
   "source": [
    "8. Usando o método `select` do Dataframe `df_nomes`, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87b63188-724a-433a-88d7-75df0e3481a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de pessoas da geração Millennials: 2272333\n"
     ]
    }
   ],
   "source": [
    "# 8. Contando o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994)\n",
    "count_millennials = df_nomes.filter((df_nomes[\"AnoNascimento\"] >= 1980) & (df_nomes[\"AnoNascimento\"] <= 1994)).count()\n",
    "print(\"Número de pessoas da geração Millennials:\", count_millennials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71422b6a-4fba-4ce1-b354-5150279a6479",
   "metadata": {},
   "source": [
    "9. Repita o processo da Pergunta 8 utilizando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c4ee6cb-8299-4beb-9a05-af4aae6e38d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de pessoas da geração Millennials (usando Spark SQL): 2272333\n"
     ]
    }
   ],
   "source": [
    "# 9. Repetindo o processo da Pergunta 8 utilizando Spark SQL\n",
    "count_millennials_sql = spark.sql(\"SELECT COUNT(*) FROM pessoas WHERE AnoNascimento >= 1980 AND AnoNascimento <= 1994\").collect()[0][0]\n",
    "print(\"Número de pessoas da geração Millennials (usando Spark SQL):\", count_millennials_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d73e9a-c84d-41bf-acc5-92c1743b9f5f",
   "metadata": {},
   "source": [
    "10. Usando Spark SQL, obtenha a quantidade de pessoas de cada país para uma das gerações abaixo. Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
    "\n",
    "   - Baby Boomers – nascidos entre 1944 e 1964;\n",
    "   - Geração X – nascidos entre 1965 e 1979;\n",
    "   - Millennials (Geração Y) – nascidos entre 1980 e 1994;\n",
    "   - Geração Z – nascidos entre 1995 e 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d265830-bfbb-4e52-a62c-0a09bf71f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de pessoas de cada país para a geração Millennials:\n",
      "+---------+------+\n",
      "|     Pais| count|\n",
      "+---------+------+\n",
      "|Argentina|189584|\n",
      "|  Bolívia|189326|\n",
      "|   Brasil|189387|\n",
      "|    Chile|189175|\n",
      "| Colômbia|189207|\n",
      "|  Equador|189140|\n",
      "|   Guiana|188921|\n",
      "| Paraguai|189481|\n",
      "|     Peru|189258|\n",
      "| Suriname|189567|\n",
      "|  Uruguai|189439|\n",
      "|Venezuela|189848|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Obtendo a quantidade de pessoas de cada país para a geração Millennials\n",
    "df_millennials = df_nomes.filter((df_nomes[\"AnoNascimento\"] >= 1980) & (df_nomes[\"AnoNascimento\"] <= 1994))\n",
    "df_millennials_grouped = df_millennials.groupBy(\"Pais\").count().orderBy(\"Pais\")\n",
    "print(\"Quantidade de pessoas de cada país para a geração Millennials:\")\n",
    "df_millennials_grouped.show(df_millennials_grouped.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
