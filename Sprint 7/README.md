<h1 align="center"> Sprint 7</h1>

<p align="center">
 <a href="#sobre">Sobre</a> ‚Ä¢
 <a href="#tarefa">Tarefa</a> ‚Ä¢
 <a href="#Laborat√≥rio">Laborat√≥rio</a> ‚Ä¢
 <a href="#desafio">Desafio</a>
</p>

<br> 

<a id="sobre"></a>
## üìé‚Ää Sobre

### Cursos e certificados
- [Hadoop, MapReduce for Big Data problems](certificados/hadoop.png)
- [Forma√ß√£o Spark com Pyspark](certificados)

### Leitura
- [Arquitetura Lambda, ETL/ELT, Bibliotecas Python NumPy e Pandas](certificados/DA-ETL-Pandas-NumPy.pdf)
- [Apache Hadoop e Apache Spark](certificados/DA-Apache+Hadoop+e+Apache+Spark.pdf)

<br>

<a id="tarefa"></a>
## üìù ‚Ää tarefa

### Python com Pandas e Numpy
- [actors.csv](evidencias/actors.csv)
- [Tarefa 1 - Resultado](evidencias/Tarefa1.ipynb)

### Apache Spark - Contador de Palavras
- [Tarefa 2 - Resultado](evidencias/Tarefa2.md)
- [output.txt](evidencias/output.txt)

<br>

<a id="Laborat√≥rio"></a>
## üë©‚Äçüíª Laborat√≥rio

### Lab AWS Glue
- [Lab - Resultado](evidencias/Lab.md)

<br>

<a id="desafio"></a>
## üéØ  Desafio 

### Tarefa: Desafio Parte 1 - ETL


**Instru√ß√µes da Tarefa**

**Ingest√£o Batch:**

A ingest√£o dos arquivos CSV ser√° realizada no Bucket Amazon S3 RAW Zone. Nesta etapa do desafio, √© necess√°rio construir um c√≥digo Python que ser√° executado dentro de um container Docker para carregar os dados locais dos arquivos para a nuvem. O processo utilizar√° principalmente a biblioteca boto3 para a ingest√£o via batch, gerando arquivos CSV.

1. **Implementar c√≥digo Python:**
    - Ler os 2 arquivos (filmes e s√©ries) no formato CSV inteiros, ou seja, sem filtrar os dados.
    - Utilizar a biblioteca boto3 para carregar os dados para a AWS.
    - Acessar a AWS e gravar no S3, no bucket definido como RAW Zone.
    - No momento da grava√ß√£o dos dados, considerar o padr√£o:
        ```
        S3://data-lake-do-fulano/Raw/Local/CSV/Movies/2022/05/02/movies.csv
        S3://data-lake-do-fulano/Raw/Local/CSV/Series/2022/05/02/series.csv
        ```
        Onde:
        - `data-lake-do-fulano` √© o nome do bucket.
        - `Raw` √© a camada de armazenamento.
        - `Local` √© a origem do dado.
        - `CSV` √© o formato do dado.
        - `Movies` ou `Series` s√£o as especifica√ß√µes do dado.
        - `2022/05/02` √© a data de processamento separada por ano/m√™s/dia.
        - `movies.csv` ou `series.csv` √© o nome do arquivo.

<br>

- [Resultado - s3_uploader.py](desafio/s3_uploader.py)

<br>

2. **Criar container Docker com um volume para armazenar os arquivos CSV e executar processo Python implementado.**

- [Resultado - dockerfile](desafio/dockerfile)

```
$ docker build -t meu_container .      
```
<img src="evidencias/build.png" alt="Texto Alternativo" width="800"> 

<br>

3. **Executar localmente o container Docker para realizar a carga dos dados ao S3.**

- [dados](desafio/dados/dados.zip)
- [.env](desafio/.env)

```
$ docker run -it -v /Users/jorgechiozzini/Desktop/Workspace/Data-Analytics-Compass/Sprint%207/desafio/dados:/root/app/dados --env-file .env meu_container 
```
<img src="evidencias/sucess.png" alt="Texto Alternativo" width="800">

<img src="evidencias/movies.png" alt="Texto Alternativo" width="800">

<img src="evidencias/series.png" alt="Texto Alternativo" width="800">

